<<setup, echo=F, cache=F, results='hide'>>=
opts_knit$set(progress = TRUE, verbose = TRUE)
timeseries = read.csv("data/data_minimal.csv")
source('R_pred_model.R')
source('R_pred_eval.R')

timeseries=timeseries[-which(timeseries$experiment == "E1" & timeseries$condition == "5a")]
@

\chapter{Prediction of Multi-episodic Judgments}\label{chap:modeling}
An initial approach on prediction of multi-episodic judgments has been conducted by \citet{moller_single-call_2011}.
Here, all prior episodic judgments are averaged as predictor.
\citet{moller_single-call_2011} evaluated the precision of this predictor with their \unit[14]{day} experiment (\cf, \autoref{prior:moeller}).
In this experiment multi-episodic judgments have been taken after the 2nd, 7th and 14th~day.
With regard to the five conditions, it could be shown that the predictor is precise for the 2nd day.
Precision decreases for later multi-episodic judgments.

This chapter starts with an overview of observed effects in the conducted experiments that are relevant for prediction of multi-episodic judgments.
Based-upon this necessary characteristics that a model must fulfill are derived.
Then, model parameters are fitted based upon \E1{}, \EIIa{}, and \E6{}.
Those experiments yielded a large data set in itself consistent data set.
All other experiments can be used for verification of an implemented model.\footnote{An approach towards modeling based upon the experiment of \citet{moller_single-call_2011} and \E5{} is published in \citet{guse_modelling_2014}.}

\section{Effects on Multi-episodic Judgments}
In the conducted experiments several effects have been observed.

First, it must be noted that multi-episodic judgment is very similar to episodic judgments, if no degraded episodes are presented.
This has been observed by \citet{moller_single-call_2011}, all one-session experiments, and field experiments.
With regard to \citet{moller_single-call_2011} and also \E4{} showed a slight increase of episodic judgments over the usage period, if no degraded episodes were presented.
However, the effect is rather small and the actual reason for this could not be deduced.
It is thus not considered for the creation of a prediction model.

%\paragraph*{Number of degraded Episodes}
Most prominent is the decrease in multi-episodic judgments, if the number of degraded usage episodes is increased (\autoref{hypo:number}).
Here, a large effect has been observed in \E1{}, \EIIa{}, and \E6{}.
In all three experiments a decrease is observed until a saturation occurred.
Here, multi-episodic judgments do not decrease further, if three consecutive \ac{LP} episode/days are presented.
Even if saturation is occurs, the multi-episodic judgment still remains above the episodic judgments of \ac{LP}.
The multi-episodic judgment is still affected by previous episodes, which were presented in \ac{HP}.
Thus, a longer integration interval must be considered.
Beside the integration interval, this indicates that one of the three \ac{LP} episodes/day did not contribute to the final multi-episodic judgment.

Beside the increase of \ac{LP} episode before a multi-episodic judgment, a position effect has been observed (\autoref{hypo:position}).
This has been investigated in \E1{}, \EIIa{}, and also \E6{}.
An impact of position and thus recency effect could be observed in \E1{} and \EIIa{}.
Here, the importance of a usage episode on a following multi-episodic judgment decreases, the more usage episodes occurred afterwards.
While \E1{}, showed such an effect in both cases, \ie, one and two \ac{LP} episodes, it was only present in \EIIa{} for two \ac{LP} episodes.
The difference can be attributed to usage situation.
In \E1{} a two-party conversation was used whereas \EIIa{} used third-party listening.
Although, not statistically significant, a recency effect was indicated in \E6{} for both cases, \ie, one or two days presented in \ac{LP}.
In \E1{} and \E6{} also the impact of non-consecutive \ac{LP} presentation was investigated.
An effect could not be observed in both experiments and thus the number of performance changes is assumed to be neglectable.

Beside number and position, recovery was investigated in \E1{} (\autoref{hypo:recovery}).
Here, additional \ac{HP} episodes are presented after a negatively affected multi-episodic judgment and the improvement assessed.
For the investigated two conditions, a recovery could observed.
Here, nine usage episodes were presented while non-\ac{HP} episodes were presented after as 5th and 6th usage episode.
The multi-episodic judgment after the 6th usage episode showed a negative effect, but the the judgments after the 3rd and 9th usage episode were not significant different.
However, in both conditions a slight negative is still indicated in the final judgment.
This finding can be explained by a recency effect, but might also due to the increased number of \ac{HP} episodes.

In the one-session experiments, also a duration neglect (\autoref{hypo:duration}) has been investigated.
In \E3{} a potential duration neglect could be shown.
Here, doubling the duration of a \ac{LP} episode did not negatively affect a following multi-episodic judgment.
In fact, even episodic judgments were not negatively affect.
It can thus be concluded that duration neglect occurs as long no macroscopic performance fluctuations occur.
Therefore, the duration of an episode must not be considered in a multi-episodic prediction model.

In one-session experiments also the independence of multi-episodic judgments between two services has been investigated (\autoref{hypo:independent}).
This has been investigated in \EIIb{}.
Here, the presentation of a second service did not affect the multi-episodic judgments of the first service.
This was found for the presentation of the second service with and without \ac{LP} episodes.
Thus, the presence of a second service must not be considered for prediction of multi-episodic judgments of one service.

\section{Model Type}
The modeling approach of \citet{moller_single-call_2011} is extended.
They proposed to the use the average of all prior episodic judgments to predict a multi-episodic judgment.
This model provides accurate prediction for earlier multi-episodic judgments, but prediction accuracy decreases, if longer sequences of multi-episodic use are predicted.

Here, a weighted average model is proposed to increase prediction accuracy. %(\autoref{eq:average})
This allows to assign an individual weight to each episodic judgment and thus model the influence on to be predicted multi-episodic judgment.
In the following, episodic judgments are denoted as $e_i$ and multi-episodic judgments are denoted as $m_n$.
$i$ denotes the usage episode and $n$ denotes the usage episode/day after which a multi-episodic judgment was taken.
The weight for a usage episode is denoted as $a_i$.
Thus, the prediction model for $\hat{m}_n$ is defined as: 

\begin{equation}\label{eq:average} %\widehat? \nicefrac
\hat{m}_n=\frac{\sum\limits_{i=1}^{n}a_i*e_i}{\sum\limits_{i=1}^{n}a_i} 
\end{equation}

In fact, the weighted average model is in itself a rather simple model, because it can only parametrized with a weighting function.
However, selecting a suiting weighting function is not a simple task, because overfitting is a serious issue.
Following Occam's Razor, two weighting functions are proposed.
Both can account for a recency effect, if desired.

The first weighting function is a window function.
This function is parametrized by the window parameter $w$.
Here, $w$ is limited to $w~\in~\mathbb{N}$ and $0~<~w~\leq~n$.
All usage episode in this window are assigned a weighting factor of 1 and all before a weighting factor of 0:
\begin{equation}\label{eq:weight:window}
a_i= \left\{
\begin{array}{lr}
  1 & : i - n + w > 0 \\
  0 & : i - n + w \leq 0
\end{array}
\right.
\end{equation}
Setting $w := n$, the model becomes an average over all prior episodic judgments.
This is in the following denoted as \emph{baseline}.

In fact, a fixed window is a rather unlikely case for an integration function.
To account for a recency effect, a linear weighting is selected.
Here, also a window is defined.
However, the weight of a usage episode decreases with the increasing distance of a usage episode to multi-episodic judgment.
To avoid overfitting $w$ is limited to $0 < w \leq 2*n+1$ and must be an odd.
\begin{equation}\label{eq:weight:linear}  %\frac{n-i}{w} & :i - n + w > 0 \\
a_i= \left\{
\begin{array}{cr}
	i-n & : i - n + w > 0 \\
  0 & : i - n + w \leq 0
\end{array}
\right.
\end{equation}

Employing a weighted average is expected to enable an increase in prediction accuracy, because a recency effect can modeled.
However, for one observed effect a weighted average will necessarily produce a deviation.
In case of the observed saturation effect both weighting functions will be too negative.
Here, three \ac{LP} episodes, or days are not judged worse than two \ac{LP}.
The implication of this is that one \ac{LP} episode is not judged different than one \ac{HP} episode with regard to the multi-episodic judgment.
This can be modeled in two ways.
On the one hand, the weight of one of the three \ac{LP} episode can be set to zero, and and the weight of \ac{HP} episodes increased.
The later would be necessary to counter a reduction due to the lower number of \ac{HP} episodes.
On the other hand, the episodic judgment of one \ac{LP} episode could be adjusted.
Here, the judgment of this episode would be changed to an \ac{HP} episode.
Then, the weighting function does not need to be changed to model this effect.
The second option seems more elegant and is more likely to reflect a characteristic of the quality formation process.

A modeling of a peak-effect is omitted here, because such an effect could not be observed in the conducted experiments.
Even, if in \C7{} of \E1{} a peak-effect occurred, then the influence on the final judgment seems to be rather small.

In the following, a \emph{model} denotes the selected weighting function with the defined parameter.

\section{Prediction Accuracy}
The prediction accuracy of a model is evaluated using two metrics.
Here, the \ac{RMSD} is used to estimate the \emph{absolute} deviation of the predicted value and actual value.
\begin{equation}\label{eq:rmsd}
RMSD: \sqrt{\frac{1}{N} \sum\limits_{i=1}^{N}(x_i-\hat{x}_i)^2}
\end{equation}

The \ac{RMSD} is complemented by the Pearson correlation.
The Pearson correlation evaluates the linear dependency of two variables.
For modeling fitting, \ie, selection of best suiting parameter, both metrics are equally used.
In a perfect case a \ac{RMSD} of 0, \ie, no deviation, and correlation coefficient of 1, \ie, perfect linear relationship, should reached.
Here, a model is preferable that explains a higher number of conditions, rather than individual conditions only.

\section{Evaluation}
In the following, modeling is done individually for each the \E1{}, \EIIa{}, and \E6{}.
This is necessary as \E1{} and \EIIa{} differ in their usage situation while \E6{} focused on a different usage period.
The evaluation is conducted based upon individual judgments, \ie, for each participants, rather than on a \ac{MOS}.
This is applied due to the limited conditions, number of participants, and between-subject design.

\subsection{One Session: E1}

\paragraph*{Recovery}
%9

\subsection{One Session: E2a}

\subsection{Multiple Days: E6}

\subsection{Conclusion}

\section{Verification}

\section{Conclusion}