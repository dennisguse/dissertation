<<setup, echo=F, cache=F, results='hide'>>=
opts_knit$set(progress = TRUE, verbose = TRUE)
source('R_plot.R')

data = read.csv("data/data_modeling.csv")
data$model_FACET = factor(data$model == "weight_linear_create", labels=c("Window (WI)", "Linear (LI)")) #label and order; FALSE will be first; then attach labels for FACET



#INDIVIDIUAL! for non-adjusted
data_nonadjusted = subset(data,  !grepl("average", data$condition) & !grepl("adjusted", data$condition) & data$experiment %in% c("E1", "E2a", "E6a"))

data_nonadjusted_avg = aggregate(data_nonadjusted$RMSD, by=list(experiment=data_nonadjusted$experiment, model=data_nonadjusted$model, parameter=data_nonadjusted$parameter, id=data_nonadjusted$id), FUN=mean)
names(data_nonadjusted_avg)[5] = "RMSD"
data_nonadjusted_avg$condition = "EXPERIMENT_average"
data_nonadjusted_avg$PEARSON = NA
data_nonadjusted_avg$model_FACET = factor(data_nonadjusted_avg$model == "weight_linear_create", labels=c("Window (WI)", "Linear (LI)")) #label and order; FALSE will be first; then attach labels for FACET
data_nonadjusted = rbind(data_nonadjusted, data_nonadjusted_avg)

data_nonadjusted$condition = factor(data_nonadjusted$condition, labels=c("C1", "C2a", "C3", "C4", "C5a", "C5b", "C6", "C7", "C8", "All"))






#We only do \ac{MOS} for non-adjusted
data_mos_nonadjusted = subset(data,  grepl("average", data$condition) & !grepl("adjusted", data$condition) & data$experiment %in% c("E1", "E2a", "E6a"))

data_mos_nonadjusted_avg = aggregate(data_mos_nonadjusted$RMSD, by=list(experiment=data_mos_nonadjusted$experiment, model=data_mos_nonadjusted$model, parameter=data_mos_nonadjusted$parameter, id=data_mos_nonadjusted$id), FUN=mean)
names(data_mos_nonadjusted_avg)[5] = "RMSD"
data_mos_nonadjusted_avg$condition = "EXPERIMENT_average"
data_mos_nonadjusted_avg$PEARSON = NA
data_mos_nonadjusted_avg$model_FACET = factor(data_mos_nonadjusted_avg$model == "weight_linear_create", labels=c("Window (WI)", "Linear (LI)")) #label and order; FALSE will be first; then attach labels for FACET
data_mos_nonadjusted = rbind(data_mos_nonadjusted, data_mos_nonadjusted_avg)

data_mos_nonadjusted$condition = factor(data_mos_nonadjusted$condition, labels=c("C1", "C2a", "C3", "C4", "C5a", "C5b", "C6", "C7", "C8", "All"))

#We only do \ac{MOS} for adjusted!
data_mos_adjusted_only=subset(data, data$condition %in% c("5a_average", "5b_average", "6_average", "6_adjusted_average"))
data_mos_adjusted_only$condition = factor(data_mos_adjusted_only$condition, labels=c("C5a", "C5b", "C6 (adjusted)", "C6"))

data_mos_adjusted=subset(data,  grepl("average", data$condition) & data$condition != "6_average" & data$experiment %in% c("E1", "E2a", "E6a"))

data_mos_adjusted_avg = aggregate(data_mos_adjusted$RMSD, by=list(experiment=data_mos_adjusted$experiment, model=data_mos_adjusted$model, parameter=data_mos_adjusted$parameter, id=data_mos_adjusted$id), FUN=mean)
names(data_mos_adjusted_avg)[5] = "RMSD"
data_mos_adjusted_avg$condition = "EXPERIMENT_average"
data_mos_adjusted_avg$PEARSON = NA
data_mos_adjusted_avg$model_FACET = factor(data_mos_adjusted_avg$model == "weight_linear_create", labels=c("Window (WI)", "Linear (LI)")) #label and order; FALSE will be first; then attach labels for FACET
data_mos_adjusted = rbind(data_mos_adjusted, data_mos_adjusted_avg)

data_mos_adjusted$condition = factor(data_mos_adjusted$condition, labels=c("C1", "C2a", "C3", "C4", "C5a", "C5b", "C6", "C7", "C8", "All"))
@

\chapter{Prediction of Multi-episodic Judgments}\label{chap:modeling}
An initial approach on prediction of multi\=/episodic judgments has been conducted by \citet{moller_single-call_2011}.
Here, all prior episodic judgments are averaged as predictor.
\citet{moller_single-call_2011} evaluated the precision of this predictor with their \unit[14]{day} experiment (\cf, \autoref{prior:moeller}).
In this experiment multi\=/episodic judgments have been taken after the 2nd, 7th and 14th~day.
With regard to the five conditions, it could be shown that the predictor is precise for the 2nd day.
Precision decreases for later multi\=/episodic judgments.
However, results are limited due to the limited impact on multi\=/episodic judgments of the conditions, \ie, presentation of \ac{LP} episodes.

This chapter starts with an overview of observed effects in the conducted experiments that are relevant for prediction of multi\=/episodic judgments.
Following, model types are presented that seem suited for successfully predicting multi\=/episodic judgments.
The prediction accuracy for those model types is evaluated using \E1{}, \EIIa{}, and \E6{}.
Those experiments yielded a large and in itself consistent data set.\footnote{An approach towards modeling based upon the experiment of \citet{moller_single-call_2011} and \E5{} is published in \citet{guse_modelling_2014}. However, these two experiments are omitted for modeling here.}
All other conducted experiments are not considered suitable for modeling, because only a very limited set of conditions was investigated.

%All other experiments can be used for verification of an implemented model.

\section{Effects on Multi-episodic Judgments}
In the conducted experiments several effects have been observed.
First, it must be noted that multi\=/episodic judgments are very similar to episodic judgments, if no degraded episodes are presented.
This has been observed by \citet{moller_single-call_2011}, and also in all here conducted experiments.
If no \ac{LP} episodes were presented the experiment of \citet{moller_single-call_2011} and \E4{} indicated a slight increase of episodic judgments over the usage period.
However, the indicated effect is rather small and the actual reason for this could not be deduced.
It is thus not considered for the implementation of a prediction model.

%\paragraph*{Number of degraded Episodes}
The largest observed effect on multi\=/episodic judgments is the decrease due to an increased number of degraded usage episodes (\autoref{hypo:number}).
This has been observed in \E1{}, \EIIa{}, and \E6{}.
Here, a decrease is observed until saturation occurred.
Then multi\=/episodic judgments do not decrease further.
This occurred, if more than two \ac{LP} episodes/days were presented.
In fact, multi\=/episodic judgments still remain above the episodic judgments of \ac{LP}.
This shows that multi\=/episodic judgments are still affected by previous \ac{HP} episodes.
This indicates a longer integration interval than 3~usage episodes or 3~days, respectively.
Beside the integration interval, this indicates that one of the three \ac{LP} episodes/days did not contribute to the final multi\=/episodic judgment.

Beside the increase of \ac{LP} episode before a multi\=/episodic judgment, a position effect has been observed (\autoref{hypo:position}).
This has been investigated in \E1{}, \EIIa{}, and also \E6{}.
An impact of position and thus a recency effect could be observed in \E1{} and \EIIa{}.
Here, the impact of \ac{LP} episode(s) on a following multi\=/episodic judgment decreases, the more \ac{HP} episodes are presented afterwards.
While \E1{}, showed such an effect in both cases, \ie, for one and two \ac{LP} episodes, it was only present in \EIIa{} for two \ac{LP} episodes.
This might be attributed to the passive usage situation.
In \E1{} a two-party conversation was used whereas \EIIa{} applied a third-party listening task.
Although not statistically significant, a recency effect was indicated in \E6{} for both cases, \ie, one or two days presented in \ac{LP}.

In \E1{} and \E6{} also the impact of non-consecutive \ac{LP} presentation was investigated (\autoref{hypo:consecutive}).
An effect could not be observed in both experiments and thus the number of performance changes is assumed to be neglectable for prediction.
In fact, the small difference might also be attributed to a recency effect.

A peak effect is not considered for modeling, because such an effect could not be observed in the conducted experiments (\autoref{hypo:strength}).
Even, if in \C7{} of \E1{} a peak effect occurred, the influence on the final judgment seems to be rather small.

In \E1{} also the recovery of a negatively affected multi\=/episodic judgment was investigated (\autoref{hypo:recovery}).
%Here, additional \ac{HP} episodes are presented after a negatively affected multi\=/episodic judgment and the improvement assessed.
This was studied in two conditions for one session.
Here, nine usage episodes were presented while non-\ac{HP} episodes were presented after as 5th and 6th usage episode.
The multi\=/episodic judgment after the 6th usage episode showed a negative effect, but the judgments after the 3rd and 9th usage episode were not significant different.
However, in both conditions a slight negative impact is indicated on the final judgment.
This finding can be explained by a recency effect but also due to the overall increased number of \ac{HP} episodes.

For one-session also a duration neglect was investigated (\autoref{hypo:duration}). 
This was investigated in \E3{}.
Here, doubling the duration of a \ac{LP} episode did not negatively affect a following multi\=/episodic judgment.
In fact, even episodic judgments were not negatively affected.
It is thus concluded that duration neglect was observed.
Therefore, the duration of an episode is not considered for prediction.

Finally, in \EIIb{} the independence of multi\=/episodic judgments of two services has been investigated (\autoref{hypo:independent}).
Here, the presentation of a second service did not affect the multi\=/episodic judgments of the first service.
This was found for the presentation of the second service with and without \ac{LP} episodes.
Thus, the presence of a second service must not be considered for prediction of multi\=/episodic judgments of one service.

\section{Types of Model}
The modeling approach of \citet{moller_single-call_2011} is extended.
They proposed to the use the average of all prior episodic judgments to predict a multi\=/episodic judgment.
This model provides accurate prediction for earlier multi\=/episodic judgments.
However, prediction accuracy decreases, if longer sequences of multi\=/episodic use are to be predicted.

In this thesis, a weighted average model is proposed to increase prediction accuracy. %(\autoref{eq:average})
This allows to assign an individual weight to each episodic judgment and thus model the impact on a multi\=/episodic judgment that is to be predicted.
In the following, episodic judgments are denoted as~$\mathit{e_i}$ and multi\=/episodic judgments are denoted as~$\mathit{m_n}$.
$\mathit{i}$~denotes the usage episode and $\mathit{n}$~denotes the usage episode after which a multi\=/episodic judgment was taken.
The weight for a usage episode is denoted as~$\mathit{a_i}$.
Thus, the prediction model for~$\mathit{\hat{m}_n}$ is defined as: 
\begin{equation}\label{eq:average}
\hat{m}_n=\frac{\sum\limits_{i=1}^{n}a_i*e_i}{\sum\limits_{i=1}^{n}a_i} \, .
\end{equation}

The weighted average model is in itself a rather simple model, because it is only parametrized with a weighting function.
However, selecting a suitable weighting function is not a simple task, because overfitting is an issue.
Following Occam's Razor, a lower degree of freedom for a weighting function is preferable.
Here, two weighting functions are proposed, which both can account for a recency effect.

The first weighting function is a window function, which is in the following denoted as~\emph{WI}.
This function is parametrized by the window parameter~$\mathit{w}$.
All episodic judgments of usage episode in this window are assigned a weighting factor of 1 and all before a weighting factor of 0 (\autoref{eq:weight:window}).
\begin{equation}\label{eq:weight:window}
WI: a_i= \left\{
\begin{array}{ll}
%  1 & : i - n + w > 0 \\
%  0 & : i - n + w \leq 0
  1,& \text{if } i - n + w > 0 \\
  0,& \text{otherwise}
\end{array}
\right.
\end{equation}
Here, $\mathit{w}$ is limited to $\mathit{w}~\in~\mathbb{N}$ and $0~<~\mathit{w}~\leq~\mathit{n}$.
Setting $\mathit{w := n}$, this model type becomes the average over all prior episodic judgments, \ie, the model proposed by \citet{moller_single-call_2011}.

In fact, a static window is a rather unlikely case to weighting, because the importance of episodic judgments is considered binary.
To overcome this a linear weighting function, denoted as \emph{LI}, is proposed here.
Here, the weight for usage episodes decreases linearly with an increasing distance to the multi\=/episodic judgment (\autoref{eq:weight:linear}).
\begin{equation}\label{eq:weight:linear}  %\frac{n-i}{w} & :i - n + w > 0 \\
LI: a_i= \left\{
\begin{array}{ll}
%	i - n + w & : i - n + 2*w > 0 \\
%  0 & : i - n + 2*w \leq 0
	i - n + w,& \text{if } i - n + 2*w > 0 \\
  0,& \text{otherwise}
\end{array}
\right.
\end{equation}
Here, $\mathit{w}$ is also limited to $\mathit{w}~\in~\mathbb{N}$ and $0~<~\mathit{w}~\leq~\mathit{n}$.
However, the actual window is increased by $\mathit{2*w}$, so the very first episodic judgment can be considered with a maximum weight of $\geq 0.5$.\footnote{Please note that normalization due to the weighting function is handled by the weighting average.}
Limiting $\mathit{w}$ for both models to the same values, allows to compare the accuracy of both models directly.

Employing a weighted average is expected to increase prediction accuracy, because a recency effect can modeled.
However, for one observed effect a weighted average will necessarily produce a deviation.
In case of the observed saturation effect, both weighting functions will be too negative.
Here, three \ac{LP} episodes/days are not judged worse than two.
This implies that one \ac{LP} episode/day is not judged different than one \ac{HP} episode with regard to the multi\=/episodic judgment.
This can be modeled in two ways.
On the one hand, the weight of one of the three \ac{LP} episodes/days can be set to zero, and the weight of \ac{HP} episodes increased.
The later would be necessary due to the reduced number of \ac{HP} episodes.
On the other hand, the episodic judgment of one \ac{LP} episode/day can be adjusted.
Here, the episodic judgment(s) of this episode/day would be changed to a \ac{HP}.
This omits adjusting the weighting function to cover this effect.
The second option seems more elegant and is more likely to reflect a characteristic of the quality formation process.

\subparagraph*{Prediction Accuracy}
In the following, a \emph{model} denotes the selected weighting function with the defined parameter $\mathit{w}$.
The prediction accuracy of a model is evaluated using the \ac{RMSD}:
\begin{equation}\label{eq:rmsd}
RMSD: \sqrt{\frac{1}{N} \sum\limits_{i=1}^{N}(x_i-\hat{x}_i)^2} \, .
\end{equation}

In a perfect case, a \ac{RMSD}~of~zero, \ie, no deviation, can be achieved.
Beside a low \ac{RMSD}, a lower $\mathit{w}$ is preferable, because this reduces the amount of information available to a model.
Overall, a model is preferable that explains a higher number of conditions rather than individual conditions only.
%The \ac{RMSD} is complemented by the Pearson correlation.
%The Pearson correlation evaluates the linear dependency of two variables.
%For modeling fitting, \ie, selection of best suitable parameter, both metrics are equally used.
%In a perfect case a \ac{RMSD}~of~0, \ie, no deviation, and correlation coefficient~$R$~of~1, \ie, perfect linear relationship, should reached.

\section{Evaluation}
%The evaluation is conducted based upon individual judgments, \ie, for each participants, rather than on a \ac{MOS}.
%This is applied due to the limited conditions, number of participants, and between-subject design.

In the following, modeling is done individually for \E1{}, \EIIa{}, and \E6{}.
This is necessary as \E1{} and \EIIa{} differ in their usage situation while in \E6{} a different usage period, \ie, 6~days, was investigated.
Modeling is conducted using the \ac{MOS}.
For each experiment first the multi\=/episodic judgment for \ac{HP}-only usage episodes is evaluated.
Following, the prediction accuracy is evaluated for multi\=/episodic judgments affected by \ac{LP} episode(s).
Afterwards, the improvement due to accounting for a saturation effect is investigated for all experiments together.

\subsection{One Session}\label{results:prediction}

\subparagraph*{Experiment \E1{}}
In \E1{}, a two-party conversation was investigated with 6 to 9 usage episodes.
With regard to the multi\=/episodic judgment after the 3rd usage episode, \ie, \ac{HP} only, both model types perform very similar.
\autoref{fig:pred:E1base} shows the \ac{RMSD} for both model types with regard to $\mathit{w}$ for each condition.
The black dashed line represents the average \ac{RMSD} for this experiment.
For both model types the accuracy remains nearly constant independent of $\mathit{w}$.
It is notable that the accuracy depends on the condition, \ie, \C7{} yields a very low \ac{RMSD} while \C1{} is far higher.
This is likely an artifact due the between-subject design.

\begin{figure}[h]
	\centering
<<plotE1BASE, echo=F, fig.height=3>>=
ggplot_model_create(subset(data_mos_nonadjusted, data_mos_nonadjusted$experiment == "E1" & data_mos_nonadjusted$id == 3), 0.26) + 
 geom_line(aes(x=parameter, y=RMSD, linetype=condition, size=condition, colour=condition)) + facet_grid(. ~ model_FACET) + 
 scale_linetype_manual(values=c("solid", "solid", "solid", "solid", "solid", "solid", "solid", "dashed")) +
 scale_color_manual(values=c("#00BFC4", "#00BE67", "#7CAE00", "#00A9FF", "#CD9600", "#F8766D", "#C77CFF", "#000000")) + 
 scale_size_manual(values=c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1))

ggplot_model_create(subset(data_nonadjusted, data_nonadjusted$experiment == "E1" & data_nonadjusted$id == 3), 0.75) + 
 geom_line(aes(x=parameter, y=RMSD, linetype=condition, size=condition, colour=condition)) + facet_grid(. ~ model_FACET) + 
 scale_linetype_manual(values=c("solid", "solid", "solid", "solid", "solid", "solid", "solid", "dashed")) +
 scale_color_manual(values=c("#00BFC4", "#00BE67", "#7CAE00", "#00A9FF", "#CD9600", "#F8766D", "#C77CFF", "#000000")) + 
 scale_size_manual(values=c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1))
@
	\caption[One session (\E1{}): prediction accuracy for \ac{HP} only episodes]{One session (\E1{}): prediction accuracy for \ac{HP} only episodes (3rd usage episode).}
	\label{fig:pred:E1base}
\end{figure}



With regard to the multi\=/episodic judgment after the 6th usage episode, both model types perform slightly differently.
\autoref{fig:pred:E1pred6} shows the \ac{RMSD} for the 6th usage episode for both model types.
Here, WI performs better while increasing $\mathit{w}=4$.
However, the prediction accuracy depends on the considered condition.
For example, \C3{} is far off for $\mathit{w}=1$ and improves until $\mathit{w}=3$ while \C4{} is best predicted with~$\mathit{w}=5$.
LI outperforms WI in prediction accuracy and is, furthermore, more consistent.
The best accuracy is observed for $\mathit{w}=2$.
All conditions except \C4{} and \C6{} yield here the minimal \ac{RMSD}.
In fact, \C4{} and \C6{} reach their minimum at~$\mathit{w}=3$.
It must be noted that \ac{RMSD} is in the best case close to \Sexpr{round(min(subset(data_mos_nonadjusted, data_mos_nonadjusted[["experiment"]] == "E1" & data_mos_nonadjusted[["id"]] == 6 & data_mos_nonadjusted[["model"]] == "weight_linear_create" & data_mos_nonadjusted[["condition"]] == "All")[["RMSD"]]), 2)} (LI, $\mathit{w}=2$).
This is close to the prediction accuracy for presenting \ac{HP} only.

\begin{figure}[h]
	\centering
<<plotE1PRED, echo=F, fig.height=3>>=
ggplot_model_create(subset(data_mos_nonadjusted, data_mos_nonadjusted$experiment == "E1" & data_mos_nonadjusted$id == 6), 1.9) + 
 geom_line(aes(x=parameter, y=RMSD, linetype=condition, size=condition, colour=condition)) + facet_grid(. ~ model_FACET) + 
 scale_linetype_manual(values=c("solid", "solid", "solid", "solid", "solid", "solid", "solid", "dashed")) +
 scale_color_manual(values=c("#00BFC4", "#00BE67", "#7CAE00", "#00A9FF", "#CD9600", "#F8766D", "#C77CFF", "#000000")) + 
 scale_size_manual(values=c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1))

ggplot_model_create(subset(data_nonadjusted, data_nonadjusted$experiment == "E1" & data_nonadjusted$id == 6), 2.0) + 
 geom_line(aes(x=parameter, y=RMSD, linetype=condition, size=condition, colour=condition)) + facet_grid(. ~ model_FACET) + 
 scale_linetype_manual(values=c("solid", "solid", "solid", "solid", "solid", "solid", "solid", "dashed")) +
 scale_color_manual(values=c("#00BFC4", "#00BE67", "#7CAE00", "#00A9FF", "#CD9600", "#F8766D", "#C77CFF", "#000000")) + 
 scale_size_manual(values=c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1))
@
	\caption[One session (\E1{}): prediction accuracy for all conditions]{One session (\E1{}): prediction accuracy for all conditions (6th usage episode).}
	\label{fig:pred:E1pred6}
\end{figure}

With regard to the recovery (\autoref{hypo:recovery}) two conditions were investigated.
The \ac{RMSD} is shown for both weighting functions in \autoref{fig:pred:E1pred9}.
In this case, LI is preferable over WI.
In fact, the WI is very precise for $\mathit{w}=4$, but is otherwise far off.
LI performs well for~\CVb{} with $\mathit{w} \geq 3$ and performs for all $\mathit{w}$ very similar for \C7{}.
Thus, the linear weight is preferable, because it has a higher robustness.

\begin{figure}[h]
	\centering
<<plotE1PRED9, echo=F, fig.height=3>>=
ggplot_model_create(subset(data_mos_nonadjusted, data_mos_nonadjusted$experiment == "E1" & data_mos_nonadjusted$id == 9 & data_mos_nonadjusted$condition != "All"), 0.75) + 
	geom_line(aes(x=parameter, y=RMSD, colour=condition)) + facet_grid(. ~ model_FACET)  + 
  scale_size_manual(values=c(0.5, 0.5))
  
ggplot_model_create(subset(data_nonadjusted, data_nonadjusted$experiment == "E1" & data_nonadjusted$id == 9 & data_nonadjusted$condition != "All"), 1.0) + 
	geom_line(aes(x=parameter, y=RMSD, colour=condition)) + facet_grid(. ~ model_FACET)  + 
  scale_size_manual(values=c(0.5, 0.5))
@
	\caption[One session (\E1{}): prediction accuracy for recovery]{One session (\E1{}): prediction accuracy for recovery (9th usage episode).}
	\label{fig:pred:E1pred9}
\end{figure}

\subparagraph*{Experiment \EIIa{}}
This experiment complemented \E1{} with a passive usage situation while sharing the same performance levels.
With regard to prediction of \ac{HP} only usage episodes, results are slightly different than for \E1{}.
The prediction accuracy for the multi\=/episodic judgment after the 3rd usage episode is shown in \autoref{fig:pred:E2pred3}.
Here, the prediction accuracy improves for an increasing $\mathit{w}$.
However, this is due to strong deviation of~\C4{} alone. 
Omitting \C4{} shows similar results to~\E1{}, \ie, prediction accuracy is not affected by $\mathit{w}$.
In fact, \ac{RMSD} reaches a similar  value than for~\E1{}.
Here, no difference between~WI and~LI is observed.

\begin{figure}[h]
	\centering
<<plotE2BASE, echo=F, fig.height=3>>=
ggplot_model_create(subset(data_mos_nonadjusted, data_mos_nonadjusted$experiment == "E2a" & data_mos_nonadjusted$id == 3), 0.4) + 
 geom_line(aes(x=parameter, y=RMSD, linetype=condition, size=condition, colour=condition)) + facet_grid(. ~ model_FACET) + 
 scale_linetype_manual(name="Condition", values=c("solid", "solid", "solid", "solid", "solid", "solid", "solid", "dashed")) +
 scale_color_manual(name="Condition", values=c("#00BFC4", "#00BE67", "#7CAE00", "#00A9FF", "#CD9600", "#F8766D", "#C77CFF", "#000000")) + 
 scale_size_manual(name="Condition", values=c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1))
 
ggplot_model_create(subset(data_nonadjusted, data_nonadjusted$experiment == "E2a" & data_nonadjusted$id == 3), 1.0) + 
 geom_line(aes(x=parameter, y=RMSD, linetype=condition, size=condition, colour=condition)) + facet_grid(. ~ model_FACET) + 
 scale_linetype_manual(name="Condition", values=c("solid", "solid", "solid", "solid", "solid", "solid", "solid", "dashed")) +
 scale_color_manual(name="Condition", values=c("#00BFC4", "#00BE67", "#7CAE00", "#00A9FF", "#CD9600", "#F8766D", "#C77CFF", "#000000")) + 
 scale_size_manual(name="Condition", values=c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1))
 @
	\caption[One session (\EIIa{}): prediction accuracy for \ac{HP} only episodes]{One session (\EIIa{}): prediction accuracy for \ac{HP} only episodes (3rd usage episode).}
	\label{fig:pred:E2pred3}
\end{figure}

With regard to the prediction of the multi\=/episodic judgment after the 6th usage episode, results closely resemble \E1{}.
The \ac{RMSD} is shown in \autoref{fig:pred:E2pred6}.
For WI a minimum is reached at $\mathit{w}=3$ and $\mathit{w}=4$ (\Sexpr{round(min(subset(data_mos_nonadjusted, data_mos_nonadjusted[["experiment"]] == "E2a" & data_mos_nonadjusted[["id"]] == 6 & data_mos_nonadjusted[["model"]] == "weight_window_create" & data_mos_nonadjusted[["condition"]] == "All")[["RMSD"]]), 2)}).
For LI the minimal \ac{RMSD} is achieved at $\mathit{w}=2$ (\Sexpr{round(min(subset(data_mos_nonadjusted, data_mos_nonadjusted[["experiment"]] == "E2a" & data_mos_nonadjusted[["id"]] == 6 & data_mos_nonadjusted[["model"]] == "weight_linear_create" & data_mos_nonadjusted[["condition"]] == "All")[["RMSD"]]), 2)}).
In fact, the found parameters are identical to \E1{}.
Furthermore, even the \ac{RMSD} behaves similar for individual conditions.

\begin{figure}[h]
	\centering
<<plotE2PRED, echo=F, fig.height=3>>=
ggplot_model_create(subset(data_mos_nonadjusted, data_mos_nonadjusted$experiment == "E2a" & data_mos_nonadjusted$id == 6), 1.8) + 
	geom_line(aes(x=parameter, y=RMSD, linetype=condition, size=condition, colour=condition)) + facet_grid(. ~ model_FACET) + 
	scale_linetype_manual(name="Condition", values=c("solid", "solid", "solid", "solid", "solid", "solid", "solid", "dashed")) +
	scale_color_manual(name="Condition", values=c("#00BFC4", "#00BE67", "#7CAE00", "#00A9FF", "#CD9600", "#F8766D", "#C77CFF", "#000000")) + 
	scale_size_manual(name="Condition", values=c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1))
	
ggplot_model_create(subset(data_nonadjusted, data_nonadjusted$experiment == "E2a" & data_nonadjusted$id == 6), 1.8) + 
	geom_line(aes(x=parameter, y=RMSD, linetype=condition, size=condition, colour=condition)) + facet_grid(. ~ model_FACET) + 
	scale_linetype_manual(name="Condition", values=c("solid", "solid", "solid", "solid", "solid", "solid", "solid", "dashed")) +
	scale_color_manual(name="Condition", values=c("#00BFC4", "#00BE67", "#7CAE00", "#00A9FF", "#CD9600", "#F8766D", "#C77CFF", "#000000")) + 
	scale_size_manual(name="Condition", values=c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1))
 @
	\caption[One session (\EIIa{}): prediction accuracy for all conditions]{One session (\EIIa{}): prediction accuracy for all conditions (6th usage episode).}
	\label{fig:pred:E2pred6}
\end{figure}

\subparagraph*{Conclusion}
For prediction of multi\=/episodic judgments in one session, both model types perform quite well.
For both experiments it is notable that the same parameters were found for the two weighting functions, although the usage situation is different.
With regard to prediction accuracy LI is preferable over~WI.
Besides, linear weighting seems to be more robust against selected parameter.
For both experiments a minimal \ac{RMSD} could be achieved for $\mathit{w}=2$.
The multi\=/episodic judgments after the 9th usage episode is optimal for $\mathit{w}=3$ (only \E1{}). 
However, as only two conditions were investigated, adjusting $\mathit{w}$ seems improper.

\subsection{Multiple Days: E6}
In \E6{} a usage period of six~days was investigated for use of an \ac{AoD} service.
This service was used twice per day.
In this experiment, the first three days (six usage episodes) were presented in \ac{HP} only.
Afterwards, the multi\=/episodic perceived quality is assessed.
Prediction accuracy for this judgment is shown in \autoref{fig:pred:E6pred6}.
Here, increasing $\mathit{w}$, improves prediction accuracy.
This is more prevalent for WI than for LI.
WI achieves its minimal \ac{RMSD} with $\mathit{w}=6$ while LI provides only a slight decrease after $\mathit{w}=3$.

\begin{figure}[h]
	\centering
<<plotE6BASE, echo=F, fig.height=3>>=
ggplot_model_create(subset(data_mos_nonadjusted, data_mos_nonadjusted$experiment == "E6a" & data_mos_nonadjusted$id == 8), 0.3) + 
	geom_line(aes(x=parameter, y=RMSD, linetype=condition, size=condition, colour=condition)) + facet_grid(. ~ model_FACET) + 
	scale_linetype_manual(name="Condition", values=c("solid", "solid", "solid", "solid", "solid", "solid", "dashed")) +
	scale_color_manual(name="Condition", values=c("#00BFC4", "#00BE67", "#7CAE00", "#00A9FF", "#CD9600", "#F8766D", "#000000")) + 
	scale_size_manual(name="Condition", values=c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1))
	
ggplot_model_create(subset(data_nonadjusted, data_nonadjusted$experiment == "E6a" & data_nonadjusted$id == 8), 0.7) + 
	geom_line(aes(x=parameter, y=RMSD, linetype=condition, size=condition, colour=condition)) + facet_grid(. ~ model_FACET) + 
	scale_linetype_manual(name="Condition", values=c("solid", "solid", "solid", "solid", "solid", "solid", "dashed")) +
	scale_color_manual(name="Condition", values=c("#00BFC4", "#00BE67", "#7CAE00", "#00A9FF", "#CD9600", "#F8766D", "#000000")) + 
	scale_size_manual(name="Condition", values=c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1))
@
	\caption[Multiple days (\E6{}): prediction accuracy for \ac{HP} only episodes]{Multiple days (\E6{}): prediction accuracy for \ac{HP} only episodes (3rd day).}
	\label{fig:pred:E6pred6}
\end{figure}

With regard to prediction the multi\=/episodic judgment after the 6th day, both weighting functions perform differently.
\autoref{fig:pred:E6pred12} shows the \ac{RMSD} for both weighting functions.
While LI reaches a minimal \ac{RMSD} at $\mathit{w}=4$, WI achieves its minimal \ac{RMSD} not until $\mathit{w}=8$.
In addition, LI achieves a minimal \ac{RMSD} of \Sexpr{round(min(subset(data_mos_nonadjusted, data_mos_nonadjusted[["experiment"]] == "E6a" & data_mos_nonadjusted[["id"]] == 14 & data_mos_nonadjusted[["model"]] == "weight_linear_create" & data_mos_nonadjusted[["condition"]] == "All")[["RMSD"]]), 1)} while WI only achieves a minimal \ac{RMSD} of
\Sexpr{round(min(subset(data_mos_nonadjusted, data_mos_nonadjusted[["experiment"]] == "E6a" & data_mos_nonadjusted[["id"]] == 14 & data_mos_nonadjusted[["model"]] == "weight_window_create" & data_mos_nonadjusted[["condition"]] == "All")[["RMSD"]]), 2)}.
With regard to \E6{}, LI is preferable to WI, because a higher prediction accuracy is achieved.
Furthermore, LI requires a smaller $\mathit{w}$ and provides a higher robustness for choosing $\mathit{w}$.

\begin{figure}[h]
	\centering
<<plotE6PRED, echo=F, fig.height=3>>=
ggplot_model_create(subset(data_mos_nonadjusted, data_mos_nonadjusted$experiment == "E6a" & data_mos_nonadjusted$id == 14)) + 
	geom_line(aes(x=parameter, y=RMSD, linetype=condition, size=condition, colour=condition)) + facet_grid(. ~ model_FACET) + 
	scale_linetype_manual(name="Condition", values=c("solid", "solid", "solid", "solid", "solid", "solid", "dashed")) +
	scale_color_manual(name="Condition", values=c("#00BFC4", "#00BE67", "#7CAE00", "#00A9FF", "#CD9600", "#F8766D", "#000000")) + 
	scale_size_manual(name="Condition", values=c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1))
	
ggplot_model_create(subset(data_nonadjusted, data_nonadjusted$experiment == "E6a" & data_nonadjusted$id == 14)) + 
	geom_line(aes(x=parameter, y=RMSD, linetype=condition, size=condition, colour=condition)) + facet_grid(. ~ model_FACET) + 
	scale_linetype_manual(name="Condition", values=c("solid", "solid", "solid", "solid", "solid", "solid", "dashed")) +
	scale_color_manual(name="Condition", values=c("#00BFC4", "#00BE67", "#7CAE00", "#00A9FF", "#CD9600", "#F8766D", "#000000")) + 
	scale_size_manual(name="Condition", values=c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1))
	@
	\caption[Multiple days (\E6{}): prediction accuracy for all conditions]{Multiple days (\E6{}): prediction accuracy for all conditions (6th day).}
	\label{fig:pred:E6pred12}
\end{figure}

\subsection{Saturation}\label{pred:saturation}
In all three experiments a saturation effect could be observed.
Here, multi\=/episodic judgments remain on the same level independent if two or three \ac{LP} episodes/days are presented although the multi\=/episodic stays above the episodic judgments for \ac{LP} episodes.
It is thus concluded that in this case one of the three \ac{LP} episodes did not attribute to the multi\=/episodic judgment.
One option to enhance the prediction model is to adjust the weighting function to decrease the weight of one or more \ac{LP} episode(s) and increase the weight of \ac{HP} episode(s).
The later is necessary to compensate for the lack of one \ac{HP} usage episodes/day.
However, a simpler approach can be derived by assuming that one of the \ac{LP} usage episodes/days did not only not contribute to the multi\=/episodic judgment, but rather is considered as \ac{HP} for the judgment of multi\=/episodic perceived quality, \ie, that \C5{} and \C6{} are not considered differently with regard to the integration process.
In fact, \C5{} and \C6{} only differ in the performance of the 4th usage episode (\E1{} and \EIIa{}) as well as 7th and 8th usage episode (\E6{}).
It is thus here proposed to change $\mathit{e_4}$ in case of \C6{} for \E1{} and \EIIa{}:
\begin{equation}\label{eq:saturate:modify1}
\tilde{e}_4=\nicefrac{1}{3} * \sum\limits_{i=1}^{3}e_i \, .
\end{equation}
For \E6{} the episodic judgments of $\mathit{e_7}$ and $\mathit{e_8}$ are adjusted as follows:
\begin{equation}\label{eq:saturate:modify2}
\tilde{e}_{[7,8]}= \nicefrac{1}{6}*\sum\limits_{i=1}^{6}e_i \, .
\end{equation}
%This adjustment actually modifies \C6{} to be similar to \C5{}.
This is equivalent to extending $\mathit{\hat{m}_n}$ in the case of \C6{} with the addition of the term (here only shown for \E1{} and \EIIa{}):
%\frac{a_4 * ((e_1 + e_2 + e_3)/3 - e_4)}{\sum\limits_{i=1}^{n}a_i}\, .
%\frac{a_4}{\sum\limits_{i=1}^{n}a_i}*((e_1 + e_2 + e_3)/3 - e_4)
\begin{equation}\label{eq:saturate:addition}
\nicefrac{a_4}{\sum\limits_{i=1}^{n}a_i}*(\nicefrac{\sum\limits_{i=1}^{3}e_i}{3} - e_4) \, .
\end{equation}
%\begin{equation}\label{eq:saturate:addition}
%\widehat{ms}_n = \hat{m}_n + \left\{
%\begin{array}{ll}
%  \frac{a_3 * ((e_6 + e_5 + e_4)/3 - e_3)}{\sum\limits_{i=1}^{n}a_i},& \text{if episodes 1..3:LP and 4..6:HP}\\
%  0,& \text{otherwise}
%\end{array}
%\right.
%\end{equation}


In the following, the prediction accuracy of both model types between \C5{}, \C6{}, and \C6{}~(adjusted) is presented.
For \E1{} and \EIIa{}, this is shown in \autoref{fig:pred:SAT:E1} and \autoref{fig:pred:SAT:E2a}, respectively.

\begin{figure}[h]
	\centering
<<plotE1SAT, echo=F, fig.height=3>>=
ggplot_model_create(subset(data_mos_adjusted_only, data_mos_adjusted_only$experiment == "E1" & data_mos_adjusted_only$id == 6), 1.25) + 
	geom_line(aes(x=parameter, y=RMSD, linetype=condition, size=condition, colour=condition)) + facet_grid(. ~ model_FACET) + 
	scale_linetype_manual(name="Condition", values=c("solid", "twodash", "solid")) +
	scale_color_manual(name="Condition", values=c("#00BFC4", "#7CAE00", "#7CAE00")) +
	scale_size_manual(name="Condition", values=c(0.5, 0.5, 0.5))
@
	\caption{One session (\E1{}): prediction accuracy for saturation effect.}
	\label{fig:pred:SAT:E1}
\end{figure}

For \E1{} and \EIIa{}, the adjustment results in a shift of the minimal \ac{RMSD} for the two model types.
In case of WI, the minimal \ac{RMSD} shifts from~$\mathit{w}=4$ to~$\mathit{w}=3$ for both experiments.
LI shows a similar effect.
Here, the minimal \ac{RMSD} shifts from~$\mathit{w}=3$ to~$\mathit{w}=2$.
It is notable for both experiments, that the adjustment leads to a similar shape of \ac{RMSD} for \C5{} and \C6{}~(adjusted).
Furthermore, the minimal \ac{RMSD} is reached earlier, but with increasing $\mathit{w}$ the \ac{RMSD} is worse for \C6{}~(adjusted) than \C6{}.
\begin{figure}[h]
	\centering
<<plotE2SAT, echo=F, fig.height=3>>=
ggplot_model_create(subset(data_mos_adjusted_only, data_mos_adjusted_only$experiment == "E2a" & data_mos_adjusted_only$id == 6), 1.0) + 
	geom_line(aes(x=parameter, y=RMSD, linetype=condition, size=condition, colour=condition)) + facet_grid(. ~ model_FACET) + 
	scale_linetype_manual(name="Condition", values=c("solid", "twodash", "solid")) +
	scale_color_manual(name="Condition", values=c("#00BFC4", "#7CAE00", "#7CAE00")) +
	scale_size_manual(name="Condition", values=c(0.5, 0.5, 0.5))
@
	\caption{One session (\EIIa{}): prediction accuracy for saturation effect.}
	\label{fig:pred:SAT:E2a}
\end{figure}

With regard to \E6{}, similar effects are observed.
The minimal \ac{RMSD} shifts from 9~to~6 for window and for linear from~7 to~4.
Here, \C6{}~(adjusted) also resembles \C5{} closely (see \autoref{fig:pred:SAT:E6}).

\begin{figure}[h]
	\centering
<<plotE6SAT, echo=F, fig.height=3>>=
ggplot_model_create(subset(data_mos_adjusted_only, data_mos_adjusted_only$experiment == "E6a" & data_mos_adjusted_only$id == 14), 2.0) + 
	geom_line(aes(x=parameter, y=RMSD, linetype=condition, size=condition, colour=condition)) + facet_grid(. ~ model_FACET) + 
	scale_linetype_manual(name="Condition", values=c("solid", "twodash", "solid")) +
	scale_color_manual(name="Condition",values=c("#00BFC4", "#7CAE00", "#7CAE00")) +
	scale_size_manual(name="Condition", values=c(0.5, 0.5, 0.5))
@
	\caption{Multiple days (\E6{}): prediction accuracy for saturation effect.}
	\label{fig:pred:SAT:E6}
\end{figure}

It can thus be concluded that the saturation effect can be accounted for using the proposed algorithm.
In all three experiments this resulted in a reduction of $\mathit{w}$.
%In fact, $\mathit{w}$ is reduced to the optimal found solution for each experiment, when evaluating all conditions (see \autoref{results:prediction}).
Although the \ac{RMSD} is increased in some cases, $\mathit{w}$ is reduced to the optimal solution found for all conditions without saturation adjustment (see \autoref{results:prediction}).
A reduction in \ac{RMSD} with LI for all conditions is observed (\E1{}: 0.02, \EIIa{}: 0.03, and \E6{}: 0.08).
In fact, this overall reduction is rather small and it can thus be concluded that, although accounting for saturation improves prediction accuracy, this improvement is rather small for, when considering all conditions.

%round(min(subset(data_mos_nonadjusted, data_mos_nonadjusted[["experiment"]] == "E6a" & data_mos_nonadjusted[["id"]] == 14 & data_mos_nonadjusted[["model"]] == "weight_linear_create" & data_mos_nonadjusted[["condition"]] == "All")[["RMSD"]]), 2)
%round(min(subset(data_mos_adjusted, data_mos_adjusted[["experiment"]] == "E6a" & data_mos_adjusted[["id"]] == 14 & data_mos_adjusted[["model"]] == "weight_linear_create" & data_mos_adjusted[["condition"]] == "All")[["RMSD"]]), 2)

%round(min(subset(data_mos_nonadjusted, data_mos_nonadjusted[["experiment"]] == "E1" & data_mos_nonadjusted[["id"]] == 6 & data_mos_nonadjusted[["model"]] == "weight_linear_create" & data_mos_nonadjusted[["condition"]] == "All")[["RMSD"]]), 2)
%round(min(subset(data_mos_adjusted, data_mos_adjusted[["experiment"]] == "E1" & data_mos_adjusted[["id"]] == 6 & data_mos_adjusted[["model"]] == "weight_linear_create" & data_mos_adjusted[["condition"]] == "All")[["RMSD"]]), 2)

%round(min(subset(data_mos_nonadjusted, data_mos_nonadjusted[["experiment"]] == "E2a" & data_mos_nonadjusted[["id"]] == 6 & data_mos_nonadjusted[["model"]] == "weight_linear_create" & data_mos_nonadjusted[["condition"]] == "All")[["RMSD"]]), 2)
%round(min(subset(data_mos_adjusted, data_mos_adjusted[["experiment"]] == "E2a" & data_mos_adjusted[["id"]] == 6 & data_mos_adjusted[["model"]] == "weight_linear_create" & data_mos_adjusted[["condition"]] == "All")[["RMSD"]]), 2)


\section{Conclusion}
In this chapter, two model types based upon the weighted average for predicting multi\=/episodic perceived quality were presented.
It could be shown that a weighted average using either a window function, or a linear weighting function enables to predict multi\=/episodic judgments.
Both model types perform similarly, if only \ac{HP} usage episodes are presented.
In fact, if \ac{LP} usages episodes were presented, both models outperform the unweighted average.
Comparing WI and LI, the latter shows a better prediction accuracy and higher robustness for choosing the parameter.
This is observed for all three experiments.
With regard to one-session experiments one interesting observation is made.
For both models, the highest prediction accuracy is achieved for the same $\mathit{w}$ in both experiments, \ie, LI:~$\mathit{w}=2$ and WI:~$\mathit{w}=4$.
Thus, the usage situation must actually not be considered in this case for predicting multi\=/episodic perceived quality.

In fact, it must be noted that the weighted average using rather simple weighting functions with one degree of freedom enables a decent prediction accuracy.